---
title: 'DATA 605 - Discussion #13'
author: "Zach Alexander"
date: "4/22/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require('dplyr')
require('tidyverse')
require('tidycensus')
require('ggplot2')
require('knitr')
require('kableExtra')
require('outliers')
require('lubridate')
```

***

### Multiple Regression Analysis of COVID-19 Data by U.S. County

***

Continuing my regression analysis from last week, this time around, I'll add in additional terms, including a quadratic term, dichotomous term, and a quadratic vs. dichotomous interaction term to build out a multiple regression.

From last week's analysis, it appears that population estimates were a fairly good predictor of the number of confirmed cases of COVID-19 at a county level. To make this more of a predictive model, we'll determine whether population estimates, as well as other variables can accurately predict the number of new confirmed cases for yesterday, April 21st, based on counts from previous days. If the model fits the data well, we could eventually see if we can apply this to future dates.

To do this, we'll first have to merge in some extra data. Fortunately, I have tidy'd other census data in past semesters and can read in the data file below that includes demographic, gender, age, education, income, and employment data on a county level.

***

##### Step 1: Downloading the Data

***

I decided to utilize the New York Times dataset that was made open-source at the end of March. For more information about the methodology of these confirmed case counts, you can find a [detailed explanation here](https://github.com/nytimes/covid-19-data).  

Additionally, I'll be using U.S. Census data estimates as factors for my regression model. This week, I decided to utilize population estimates as my factor for my regression. I found a large dataset on the [Census website](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html#par_textimage_70769902) with population estimates as recent as 2019.

And finally, as mentioned above, I downloaded the census data I compiled from last semester as an additional file for this week.

I saved all three of these files to my github account and read them into R:  

```{r cars}
county_covid <- read.csv('https://raw.githubusercontent.com/zachalexander/data605_cuny/master/Homework13/Discussion13/us-counties_covid19_4222020.csv', header = TRUE)

county_population <- read.csv('https://raw.githubusercontent.com/zachalexander/data605_cuny/master/Homework12/Discussion12/county_population.csv', header = TRUE)

education_data <- read.csv('https://raw.githubusercontent.com/zachalexander/data_606_cunysps/master/Final_Project/education_data_fip.csv', header = TRUE)
```

***

##### Step 2: Tidying the Data

***

With the data successfully loaded into R, I then had to do a bit of tidying to ensure that my county-level data was accurate and reflected the most up-to-date COVID-19 confirmed case counts.  

First, I decided to tidy my COVID-19 data. Since the confirmed counts are cumulative and broken out by each day since late January, I needed to filter the dataset to only include the confirmed counts for April 14th, 2020.

```{r}
covid_week <- county_covid %>% 
  arrange(fips, date) %>% 
  filter((as.Date(date) >= as.Date('2020-04-12')) & (as.Date(date) <= as.Date('2020-04-21')))

covid_test <- covid_week %>% 
  filter((as.Date(date) == as.Date('2020-04-21')))

covid_week_pivot <- covid_week %>% 
  pivot_wider(id_cols = c(date, fips, state, county), names_from = date, values_from = cases)

diff_list <- c("413_diff", "414_diff", "415_diff", "416_diff", "417_diff", "418_diff", "419_diff", "420_diff")
covid_week_pivot <- cbind(covid_week_pivot, setNames( lapply(diff_list, 
                                         function(x) x=NA), diff_list) )

for(i in 1:length(covid_week_pivot$fips)){
  for(j in 1:8){
  covid_week_pivot[i,(j+12)] <- covid_week_pivot[i, (j+4)] - covid_week_pivot[i, (j+3)]
  }
}

covid_week_fnl <- covid_week_pivot %>% 
  select(fips, state, county, `413_diff`, `414_diff`, `415_diff`, `416_diff`, `417_diff`, `418_diff`, `419_diff`, `420_diff`, `2020-04-20`)


covid_week_fnl <- mutate(covid_week_fnl, week_avg = rowMeans(covid_week_fnl[, 4:11], na.rm = TRUE))
```

Next, I noticed that the New York Times groups the confirmed counts for New York City into one row, instead of breaking it into the five counties that comprise of "New York City" (Queens, Kings, New York, Bronx and Richmond). Therefore, I decided to use the FIPS code associated with New York County (36061) as my identifier when I eventually merge the population data into the confirmed case data. I then adjusted the population estimate to reflect the population of all five counties instead of just New York County (seen later).  

```{r}
covid_week_fnl <- within(covid_week_fnl, {
    f <- county == 'New York City'
    fips[f] <- '36061'
}) 

covid_test <- within(covid_test, {
    f <- county == 'New York City'
    fips[f] <- '36061'
}) 

covid_test <- covid_test %>% 
  select(fips, state, county, cases)
```

For the merge, I also thought it would be helpful to make the county names consistent across both datasets.
```{r}
covid_week_fnl$county <- paste(covid_week_fnl$county, 'County')
```

With the COVID-19 data file ready for the merge, I then turned my attention to my population data file. In order to use county FIPS codes as my identifier in both datasets, I had to generate the FIPS codes in the population file.
```{r}
county_population <- county_population %>% 
  select(STATE, COUNTY, STNAME, CTYNAME, POPESTIMATE2019)

county_population$fips <- NA
for (i in 1:length(county_population$STATE)) {
  if(county_population$COUNTY[i] < 10) {
    county_population$fips[i] <- paste0(county_population$STATE[i], '00', county_population$COUNTY[i])
  }
  if(county_population$COUNTY[i] < 100 & county_population$COUNTY[i] >= 10) {
    county_population$fips[i] <- paste0(county_population$STATE[i], '0', county_population$COUNTY[i])
  }
  if(county_population$COUNTY[i] >=100) {
    county_population$fips[i] <- paste0(county_population$STATE[i], county_population$COUNTY[i])
  }
}

county_population <- county_population %>% 
  select(fips, CTYNAME, STNAME, POPESTIMATE2019)
names(county_population) <- c('fips', 'county', 'state', 'pop_estimate')
```

I also noticed that this file had overall state population estimates, so before merging, I made sure to filter these out.
```{r}
county_population <- county_population %>% 
  filter(grepl("County",county))
```

With both data files ready to go, I then merged the population estimates data into the COVID-19 data file and created one final dataframe. I also updated the population count for New York City to ensure that it didn't just account for the population in New York County, but also the four other counties in the metropolitan area.
```{r}
fnl <- merge(covid_week_fnl, county_population, by='fips')

fnl <- fnl %>% 
  select(fips, county.x, state.x, `2020-04-20`, week_avg, pop_estimate)

names(fnl) <- c('fips', 'county', 'state', 'April20cases', 'week_avg', 'pop_estimate')

fnl$pop_estimate[fnl$county == 'New York City County'] <- 8398748
fnl$county[fnl$county == 'New York City County'] <- 'New York City'

fnl <- fnl %>% 
  arrange(desc(week_avg))
```

Here is a look at my final dataframe, ready to start my regression analysis:  

```{r}
kable(head(fnl, n=15L)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

We'll then merge in a few more terms from the other census dataset:

```{r}
fnl <- merge(fnl, education_data, by='fips')

fnl <- fnl %>% 
  select(fips, county.x, state.x, April20cases, week_avg, pop_estimate, median_hh_inc, age65andolder_pct, nonwhite_pct, lesscollege_pct)

```

```{r}
par(mfrow=c(3,3))
hist(fnl$April20cases)
hist(fnl$week_avg)
hist(fnl$pop_estimate)
hist(fnl$median_hh_inc)
hist(fnl$age65andolder_pct)
hist(fnl$nonwhite_pct)
hist(fnl$lesscollege_pct)
```

***

##### Dichotomous Term

***

```{r}
mean(fnl$median_hh_inc)
```


It looks like we could create a dichotomous term from our median household income variable. By doing this, we'll create a new variable called `median_hh_inc_dichot`, which will be a value of 1 if the median household income is greater than $48,300 and 0 if the median household income is less than this value for each county:

```{r}
for (i in 1:length(fnl$fips)) {
  if(fnl$median_hh_inc[i] > 43000){
    fnl$median_hh_inc_dichot[i] <- 1
  } else {
    fnl$median_hh_inc_dichot[i] <- 0
  }
}
```

And here's the split between the two factors of this new dichotomous variable:
```{r}
table(fnl$median_hh_inc_dichot)
```

***

##### Quadratic Term

***

It looks like from our initial plots of histograms above, we can transform our `nonwhite_pct` variable by taking the square root, to make it more of a normal distribution. By doing this, we can now see that the distribution is approaching normal:

```{r}
par(mfrow=c(3,3))
hist(fnl$April20cases)
hist(fnl$week_avg)
hist(1 / log(fnl$pop_estimate))
hist(fnl$median_hh_inc)
hist(fnl$age65andolder_pct)
hist(sqrt(fnl$nonwhite_pct))
hist(fnl$lesscollege_pct)

fnl$pop_estimate <- 1 / log(fnl$pop_estimate)
fnl$nonwhite_pct <- sqrt(fnl$nonwhite_pct)

par(mfrow=c(3,3))
hist(fnl$April20cases)
hist(fnl$week_avg)
hist(fnl$pop_estimate)
hist(fnl$median_hh_inc)
hist(fnl$age65andolder_pct)
hist(fnl$nonwhite_pct)
hist(fnl$lesscollege_pct)
```

Additionally, I did a log transformation on the population estimate term to make it more normalized as well.

***

##### Quadratic and Dichotomous Interaction Term

***


When we run our linear regression model, we'll be sure to create an interaction term between our non-white percentage variable (quadratic transformation) and our dichotomous median household income variable.


***

##### Initial Analysis

***

With our variables and dataset ready to go, we will now take a quick look at interactions between variables. We can do this using the `pairs()` function:

```{r}
fnl_pairs <- fnl %>% 
  select(April20cases, week_avg, pop_estimate, median_hh_inc, median_hh_inc_dichot, age65andolder_pct,nonwhite_pct, lesscollege_pct)
pairs(fnl_pairs)
```

If you look closely, we can see that there seems to be a somewhat strong interaction between median household income and counties with a larger percentage of their population with less than a college degree -- this make sense intuitively. Other interactions are not as obvious, so we'll see how this fairs in our regression model.

***

##### Building the Multiple Regression

***

With our factors ready to go, we can create a multiple linear regression model.

```{r, warning=FALSE}
covid_lm <- lm(fnl$April20cases ~ fnl$April20cases + fnl$week_avg + fnl$pop_estimate + fnl$median_hh_inc + fnl$median_hh_inc_dichot + fnl$age65andolder_pct + fnl$nonwhite_pct + fnl$lesscollege_pct + fnl$median_hh_inc_dichot:fnl$nonwhite_pct)
```

Above, we can see the intercept and slope of our linear regression (8.962 and 0.5914 respectively). To get a more detailed outlook of the performance of our model, we can use the `summary()` function in R:  

```{r}
summary(covid_lm)
```
***

##### Backward Elimination

***

It looks like our `age65andolder_pct` variable has the highest p-value, and others, including `median_hh_inc` and `lesscollege_pct` will likely be removed when we do backward elimination because they also exhibit pretty high p-values. We can double check this by running the stepwise process:

```{r, warning=FALSE}
covid_lm <- step(covid_lm, direction = 'backward', trace = FALSE)
summary(covid_lm)
```

***

##### Evaluating the Model and Residual Analysis

***

After running the summary above, we can see a few things:

+ The median residual value is around zero (at 14 cases), which is a good sign.  

+ Additionally, the minimum and maximum values of the residuals are roughly around the same, albeit leaning a bit more on the minimum side of at -7192 and 4883 respectively.

+ Our Multiple R-squared value is 0.9825, which indicates that these terms by county account for about 98.25% of the variability in the number of confirmed COVID-19 cases by county for April 20th, 2020. This seems to be a pretty good result for our regression model.  


Although this seems to be a good sign, we need to check our residuals to see if this qualifies as a suitable regression model.

When plotting residuals (below), we can see that residuals are not uniformly scattered around zero:  
```{r}
plot(fitted(covid_lm),resid(covid_lm))
```

I'm not sure if this is due to some very large outliers and skewed cases data, but we can also see that there are a fair amount of outliers towards both ends of the q-q plot. This can be visualized in a quantile-versus-quantile (Q-Q) plot (see below):  

```{r}
qqnorm(resid(covid_lm))
qqline(resid(covid_lm))

hist(resid(covid_lm))
```

***

##### Was the Linear Model Appropriate?

***

From this residual analysis, we can conclude that a linear model here may not be very appropriate, or accurate for areas with larger numbers of confirmed cases (i.e. counties with larger populations). However, this may be effective to predict cases for counties with smaller populations. At this point, I think I would not call this linear model appropriate, however, further manipulation of the initial dataset to exclude counties with very large populations may make this a more effective model.


##### Predictions for April 21st, 2020

From the t-test below, it does look like a 95% confidence interval includes zero. However, it isn't a very tight range between the lower and upper bounds, indicating that the predictions for confirmed cases could be drastically off, especially for smaller populations. This confirms that it would be beneficial to adjust the original dataset to build out a more productive and efficient model.

```{r, warning=FALSE}
predicted <- predict(covid_lm, covid_test)
delta <- predicted - covid_test$cases

t.test(delta, conf.level = 0.95)
plot(delta)
```




