---
title: "DATA 605 - Final Project"
author: "Zach Alexander"
date: "5/17/2020"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(dplyr)
require(tidyr)
require(kableExtra)
require(knitr)
require(psych)
require(matrixcalc)
require(pracma)
```

***

### Instructions

Your final is due by the end of the last week of class. You should post your solutions to your GitHub account or RPubs. You are also expected to make a short presentation via YouTube and post that recording to the board. This project will show off your ability to understand the elements of the class.

### Problem 1

***
> Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6. Then generate a random variable Y that has 10,000 random normal numbers with a mean of $\mu=\sigma=\frac{(N+1)}{2}$

***

Following the instructions from above, here is my R syntax to generate a random variable X:  

```{r}
set.seed(123)

N <- 10
X <- runif(10000, 1, N)
Y <- rnorm(10000, (N+1)/2, (N+1)/2)
```

From the above syntax, you can see that I've created a variable $X$ with 10,000 random uniform numbers from 1 to $N$ ($N=10$). Additionally, I've created a random variable $Y$ that has 10,000 random normal numbers with a $\mu$ and $\sigma$ of $\frac{N+1}{2}$. 

***

> *Probability*: Calculate as a minimum the below probabilities a through c. Assume the small letter $x$ is estimated as the median of the $X$ variable, and the small letter $y$ is estimated as the 1st quartile of the $Y$ variable.  Interpret the meaning of all probabilities.  

***

To work through this, I'll first have to calculate $x$ (the median) and $y$ (the first quartile). Additionally, I've saved $X$ and $Y$ in a dataframe and stored the total number of rows (10,000) in a variable:  
```{r}
x <- quantile(X, 0.50)
y <- quantile(Y, 0.25)
df <- data.frame(X = X, Y = Y)
total_rows <- nrow(df)
```

***

> *a)* $P(X>x \ | \ X>y)$

***

We can use the following equation to find the probability:  

$$P(A \ | \ B) = \frac{P(A \ \cap \ B)}{P(B)} \\ where \ A = P(X>x) \ and \ B = P(X>y)$$  

With this in mind, we can solve:  
```{r}
P_AandB <- nrow(df %>% filter(X > x & X > y)) / total_rows
P_B <- nrow(df %>% filter(X > y)) / total_rows

P_AgivenB <- P_AandB / P_B
P_AgivenB
```

**Solution: The probability $P(X>x \ | \ X>y) = 0.5510$, which means that the probability that a uniform number from 1 to 10 is greater than the median of 5.45 *given* this number is greater than 1.84 is 0.5510.**  

***

> *b)* $P(X>x, \ Y>y)$  

***

We can use the following equation to solve this portion:  

$$P(A,B)=P(A) \times P(B) \\ where \ A=N(X>x) \ and \ B=N(Y>y)$$  

With this in mind, we can solve:  
```{r}
P_AandB_2 <- nrow(df %>% filter(X > x & Y > y)) / total_rows
P_AandB_2
```

**Solution: The probability $P(X>x, \ Y>y) = 0.3756$, which means that the probability that a uniform number from 1 to 10 is greater than the median of 5.45 *and* this number is greater than 1.84 is 0.3756.**  

***
> *c)* $P(X<x \ | \ X>y)$  

***

Similar to part a, we can use the following equation to solve:  

$$P(A \ | \ B) = \frac{P(A \ \cap \ B)}{P(B)} \\ where \ A = P(X<x) \ and \ B = P(X>y)$$  

\br

With this in mind, we can solve:  

```{r}
P_AandB <- nrow(df %>% filter(X < x & X > y)) / total_rows
P_B <- nrow(df %>% filter(X > y)) / total_rows

P_AgivenB_2 <- P_AandB / P_B
P_AgivenB_2
```

\br

**Solution: The probability $P(X<x \ | \ X>y) = 0.4490$, which means that the probability that a uniform number from 1 to 10 is less than the median of 5.45 *given* this number is greater than 1.84 is 0.4490.**  

***

> Investigate whether $P(X>x \ and \ Y>y)=P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.  

***

To do this investigation, we'll first have to calculate the joint probabilities:  

We'll calculate joint probabilities by using the equation:  

$$P(A \cap B) = P(A \times B)$$
Therefore, we first will create a few columns to distinguish whether our values meet the following conditions:  

+ Is $X > x$ or $X < x$?
+ Is $Y > y$ or $Y < y$?

Then, we can take the count of each condition, and multiply the four conditions to get their respective joint probabilities:  

+ $P((X > x) \times (Y < y))$
+ $P((X < x) \times (Y < y))$
+ $P((X < x) \times (Y > y))$
+ $P((X > x) \times (Y > y))$

Below is the R syntax I used to do this, by using `group_by()` and `summarise()` functions to do the calculations with the `tidyverse` package:  

```{r}
# Joint probabilities
prob_df <- df %>% 
  mutate(A = ifelse(X > x, " X > x", " X < x"), B = ifelse(Y > y, " Y > y", " Y < y")) %>% 
  group_by(A, B) %>% 
  summarise(count = n()) %>% 
  mutate(prob = count / total_rows)
```

Next, we need to back up a bit and examine the marginal probabilities of $P(A)$ and $P(B)$ independently:  

Intuitively we know that $x = median(X)$, therefore, half of the values of $X$ > $x$ and the other half of $X$ < $x$. Similarly, since we know that $y = first \ quartile \ of \ Y$, a quarter of $Y$ < $y$ and the remaining three fourths of the values of $Y$ > $y$. Therefore, the marginal probabilities are:  

+ $P(X > x) = 0.50$  
+ $P(X < x) = 0.50$  
+ $P(Y > y) = 0.75$  
+ $P(Y < y) = 0.25$  

We can check these assumptions by reworking our table:  

```{r}
prob_df <- prob_df %>% 
  ungroup() %>% 
  group_by(A) %>% 
  summarise(count = sum(count), prob = sum(prob)) %>% 
  mutate(B = "Total Prob") %>% 
  bind_rows(prob_df)

prob_df <- prob_df %>% 
  ungroup() %>% 
  group_by(B) %>% 
  summarise(count = sum(count), prob = sum(prob)) %>% 
  mutate(A = "Total Prob") %>% 
  bind_rows(prob_df)
```


And by printing our table, we can see that our marginal probabilities that we assumed are confirmed:  

```{r}
prob_df %>%
  dplyr::select(-count) %>% 
  spread(A, prob) %>%
  rename(" " = B) %>%
  kable() %>%
  kable_styling(bootstrap_options = c('striped'), full_width = FALSE)
```

**Solution: After generating the table above, it appears that $P(X>x \ and \ Y>y)$ does equal $P(X>x)P(Y>y)$, since they both have a value of 0.3756. By doing the calculations to check, $P(X>x)P(Y>y) = (0.50)(0.75) = 0.375$ and $P(X>x \ and \ Y>y) = 0.375$ as well.**  

***

> Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test. What is the difference between the two? Which is most appropriate?

***

In order to run our Fisher's Exact Test and Chi Square Test, we'll first need to find the counts of each our conditions, similar to the previous question and create a two-by-two table:  

```{r}
X_plus <- nrow(df %>% 
  filter(X > x))

X_lesseq <- nrow(df %>% 
  filter(X <= x))

Y_lesseq <- nrow(df %>% 
  filter(Y <= y))

Y_plus <- nrow(df %>% 
  filter(Y > y))

freq_matrix <- matrix(c(X_plus, X_lesseq, Y_plus, Y_lesseq), 
                      nrow = 2, ncol = 2, byrow = TRUE,
                      dimnames = list(c("x", "y"),
                                c("X > x; Y > y", "X <= x; Y <= y")))
```

With our frequency table ready to go, we can see our counts below:

```{r}
freq_matrix %>% 
  kable() %>%
  kable_styling(bootstrap_options = c('striped'), full_width = FALSE)
```

Now, we are ready to run our Fisher's Exact Test:  

```{r}
fisher.test(freq_matrix)
```

And here is the Chi-Square Test:  

```{r}
chisq.test(freq_matrix)
```

When looking at both of the outputs, we can see that we reject the null hypothesise for both, given that the p-value is less than 0.05 for the 95 percent confidence interval. Since the Fisher's Exact Test is typically used for smaller sample sizes, and our sample is of 10,000 observations, it would be more appropriate to use the Chi-Square Test. 

**Solution: **

+ **Our $H_0$ = There is no relationship between X and Y.**  

+ **Our $H_a$ = There is a significant relationship between X and Y.**  

**Given our outputs, independence does not hold when we run the Fisher's Exact Test and Chi-Square test. We reject the null hypothesis that there is no relationship between X and Y. Since the Fisher's Exact Test is typically used on smaller sample sizes, and we have a large sample size of 10,000 observations, it would be more appropriate to rely on the output from our Chi-Square Test. **  

***

### Instructions

You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques]([https://www.kaggle.com/c/house-prices-advanced-regression-techniques). I want you to do the following.

### Problem 2

***

> *Descriptive and Inferential Statistics*: Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?

***

```{r, warning=FALSE, message=FALSE}
kaggle_df <- read_csv('https://raw.githubusercontent.com/zachalexander/data605_cuny/master/Final%20Exam/train.csv')
```

```{r}
# summary(kaggle_df)
```

On a quick look at the summary, I decided to pull out the numeric values to explore a bit more:
```{r}
numerics <- data.frame(summary(kaggle_df))

numerics <- numerics %>% 
  filter(!is.na(Freq))

numerics <- data.frame(numerics %>% 
  filter(!grepl('Class', Freq), !grepl('Length', Freq), !grepl('Mode', Freq)) %>% 
  separate(Freq, c('Value', 'Count'), sep = ":", remove = TRUE))

numerics$Var2 <- trimws(numerics$Var2)
numerics$Value <- trimws(numerics$Value)
numerics$Count <- round(as.numeric(trimws(numerics$Count)), 1)

numerics <- numerics %>%
  dplyr::select(Var2, Value, Count) %>%
  spread(Value, Count) %>% 
  rename("Attribute" = "Var2", "Q1" = `1st Qu.`, "Q3" = `3rd Qu.`, "Max" = "Max.", "Min" = "Min.", "NA" = "NA's") %>% 
  dplyr::select(Attribute, Min, Q1, Median, Mean, Q3, Max, `NA`)

kable(numerics) %>% 
  kable_styling(bootstrap_options = 'striped')
```

With my `numerics` dataframe ready, I can now set up some visuals by splitting the dataframe into two, one with all quantative variables, and the other with categorical data:  

```{r}
kaggle_numerics <- kaggle_df[, (colnames(kaggle_df)) %in% numerics$Attribute]
kaggle_categorical <- kaggle_df[, !(colnames(kaggle_df) %in% numerics$Attribute)]
```


***

##### Exploring the quantitative variables

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(2:11) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```
```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(12:21) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(22:31) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(32:38) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

After creating a few plot matrices with our quantitative data, I thought there were a few initial takeaways:  

+ Many of the living spaces, and the square footage values seem to show stronger correlations with one another. This makes sense given that many times, the square footage of one floor will likely be similar square footage to another floor in the house.  For instance, there's a strong correlation between the first floor square footage and the basement square footage. When thinking about our regression later, it's nice to know that there are multiple attributes here that could serve as proxies (if needed).

+ Additionally, I'm seeing a gradual, positive trend towards larger areas of garages the later they are built in the 20th century -- for instance, garages built in the early part of the 20th century seem to be smaller than those that are built in the later 1900s and early 2000s. This may be something to look into further, and it's relationship with the Sale Price later on.  

+ It's interesting to see too that the year the house is built has a relationship with the rating of overall material and finish quality of the house. Again, this make sense, but it would be valueable to see if this has a relationship with Sale Price.

Now, let's explore the categorical variables a bit.   


***

##### Exploring a few of the categorical variables

```{r}
table(kaggle_categorical$LandSlope)
table(kaggle_categorical$HouseStyle)
table(kaggle_categorical$RoofMatl)
table(kaggle_categorical$CentralAir)
table(kaggle_categorical$HeatingQC)
table(kaggle_categorical$SaleType)
table(kaggle_categorical$SaleCondition)
table(kaggle_categorical$Foundation)
```

After looking through a large number of categorical values, and checking frequencies, a few things stood out to me:  

+ It looks like there's a majority of houses that are built on gentle slopes, however, there's a proportion that are built on moderate or severe slopes -- this could be something to look into further when building out the regression later on. It may be interesting to see if there is any correlation here with the Sale Price.  

+ Similarly, the foundation material seems to be split heavily between cinder blocks and poured concrete. This is another feature that may be useful to explore later to see if this has an affect on housing prices, or if there is a connection between the year a house was built and which type of foundation was used.  

+ Finally, Sale Type and Sale Condition were interesting values and their frequencies suggest that there isn't a "one-size-fits-all" approach to certain transactions, making this an enticing set of features to explore further.  

***

##### Developing a scatterplot matrix

After plotting all of my quantitative variables earlier, I thought I'd hone in on a few that may be useful to explore their relationship with the Sale Price variable. You can find my plot below: 

```{r, fig.width=12, fig.height=12}
pairs(kaggle_df[, c('1stFlrSF', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'YearBuilt', 'SalePrice' )], pch = 19)
```

*** 
##### Developing a correlation matrix  

For our correlation matrix, I will use `GrLivArea`, `1stFlrSF`, and `SalePrice`. And taking a few of these same attributes, I thought it would be interesting to plot these in a correlation matrix:  

```{r}
kaggle_numerics %>% 
  dplyr::select(`1stFlrSF`, `GrLivArea`, SalePrice) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")

corr_matrix <- kaggle_numerics %>% 
  dplyr::select(`1stFlrSF`, GrLivArea, SalePrice) %>% 
  cor() %>% 
  as.matrix()
```

As we can see from above, all three attributes, the square footage of the first floor, the above grade (ground) living area and the Sale Price all seem to have a positive relationship with one another. All three distributions also appear to be unimodal and right-skewed. Below is the official correlation matrix:  

```{r}
kable(corr_matrix) %>% 
  kable_styling(bootstrap_options = 'striped', full_width = FALSE)
```

***

##### Hypothesis testing on correlation matrix 

Now, although these correlations seem to be quite strong, we can do some hypothesis testing on their relationships at an 80-percent confidence interval:  

```{r}
corr1 <- cor.test(kaggle_numerics$SalePrice, kaggle_numerics$`1stFlrSF`, method = 'pearson', conf.level = 0.80)

corr2 <- cor.test(kaggle_numerics$SalePrice, kaggle_numerics$GrLivArea, method = 'pearson', conf.level = 0.80)

corr3 <- cor.test(kaggle_numerics$`1stFlrSF`, kaggle_numerics$GrLivArea, method = 'pearson', conf.level = 0.80)

corr1
corr2
corr3
```

**Solution: After running the three pairwise correlations between my variables of `SalePrice`, `1stFlrSF`, and `GrLivArea`, I can see from the outputs that all three accept the alternative hypothesis that the true correlation is not equal to zero and that there is a positive relationship between these attributes. Therefore, this rejects the null hypothesis that states that the correlations between each pairwise set of variables is zero. I know this to be true since the p-values for all three of my Person's correlation tests are less than 0.05.**  

**Solution: Additionally, since we are dealing with a large sample size, and our correlation tests are yielding very small p-values, all less than 0.001, I'm not worried about family-wise error when measuring relationships across these three attributes. With p-values this low, it creates a solid argument that we aren't running into a Type 1 error here.**  

***

> *Linear Algebra and Correlation*: Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. 

***

Remembering that the inverse of a matrix multiplied by a matrix will become the identity matrix: 

$$A^{−1}A=AA^{−1}=I$$

We can find our precision matrix by using the `solve()` function in baseR to invert our correlation matrix:  

```{r}
prec_matrix <- solve(corr_matrix)
prec_matrix
```

Then, we can multiple the precision matrix by the correlation matrix to confirm that we get the identity matrix:  

$$ correlation \ matrix \times \ precision \ matrix = $$

\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
\end{bmatrix}

```{r}
I_1 <- round(prec_matrix %*% corr_matrix, 2)
I_2 <- round(corr_matrix %*% prec_matrix, 2)

I_1
I_2
```

As we can see above, we do indeed get the Identity Matrix when we multiply the precision matrix with the correlation matrix and vice-versa.  

***

Finally, we will perform LU decomposition on the matrix. Fortunately, R has a package called `lu.decomposition()` that will find the upper and lower triangle matrices of our precision matrix, $U$ and $L$ respectively. The relationship between these two matrices is based on the formula:

$$A = LU$$
Where $A$ = our precision matrix, and $L$ and $U$ are the upper and lower triangular matrices of our precision matrix. We can see, when finding $L$ and $U$ below, we get these respective matrices:
```{r}
A <- prec_matrix
L <- lu.decomposition(A)$L
U <- lu.decomposition(A)$U

round(L, 2)
round(U, 2)
```

And when we multiply these together, we can obtain our original matrix ($A$):  

```{r}
round(A, 2)
round(L %*% U, 2)
```


\begin{equation}
  LU = A, where
  \space
  A = 
  \begin{bmatrix}
    1.68 & -0.46 & -0.69\\
    -0.46 & 2.14 & -1.23\\
    -0.69 & -1.23 & 2.29\\
  \end{bmatrix}
  \space
  = 
  \space
  \begin{bmatrix}
    1.00 & 0.00 & 0.00\\
    -0.27 & 1.00 & 0.00\\
    -0.41 & -0.71 & 1.00\\
  \end{bmatrix}
  \space
    \begin{bmatrix}
    1.68 & -0.46 & -0.69\\
    0.00 & 2.01 & -1.42\\
    0.00 & 0.00 & 1.00\\
  \end{bmatrix}
  \space
  = LU 
\end{equation}



***

> *Calculus-Based Probability & Statistics*: Many times, it makes sense to fit a closed form distribution to data. Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary. Then load the MASS package and run fitdistr to fit an exponential probability density function. (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ). Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$)).  Plot a histogram and compare it with a histogram of your original variable. Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF). Also generate a 95% confidence interval from the empirical data, assuming normality. Finally, provide the empirical 5th percentile and 95th percentile of the data. Discuss.

***

For this exercise, we'll use the `GrLivArea` variable from the training dataset. When we plot it below on a histogram we can quickly see that it is skewed to the right:  

```{r, message=FALSE, warning=FALSE}
ggplot(kaggle_df, aes(GrLivArea)) +
  geom_histogram(aes(y = ..density..), colour="#555555", fill="#dddddd") +
  geom_density(alpha=.6, fill="#333333") +
  geom_vline(xintercept = min(kaggle_df$GrLivArea)) +
  labs(title = "Above Ground Living Area (square feet)", align='middle') +
  theme_minimal()
```

We can also see that the minimum value is absolutely above zero, so we do not need to perform any shifts. We can double check the data by using the `summary()` function, and we see that the minimum value is 334 square feet:  

```{r}
summary(kaggle_df$GrLivArea)
```

*** 

##### Loading the MASS package and running fitdistr()

After loading the `MASS` package and running `fitdistr()` to fit an exponential probability density function to our variable, I've found that that the optimal value of $\lambda$ for this distribution is 0.000659864.

```{r, warning=FALSE, message=FALSE}
require(MASS)

exp_fit <- fitdistr(kaggle_df$GrLivArea, "exponential")
est_rate <- exp_fit$estimate
est_rate
```

***

##### Plot a histogram comparing the exponential distribution to the original

Using my newly found optimal value for lambda, I then was able to generate a 1000-sample variable from this exponential distribution:  

```{r, warning=FALSE, message=FALSE}
set.seed(123)
pdf <- rexp(1000, est_rate)
pdf_df <- as.data.frame(pdf)
```

I then plotted this distribution below, along with my original distribution for `GrLivArea`:
```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(pdf_df, aes(pdf_df$pdf)) +
  geom_histogram(aes(y = ..density..), colour="#555555", fill="#dddddd") +
  geom_density(alpha=.6, fill="#333333") +
  geom_vline(xintercept = min(pdf_df$pdf)) +
  labs(title = "Exponential Distribution of Above Ground Living Area (square feet)", align='middle') +
  theme_minimal()

ggplot(kaggle_df, aes(GrLivArea)) +
  geom_histogram(aes(y = ..density..), colour="#555555", fill="#dddddd") +
  geom_density(alpha=.6, fill="#333333") +
  geom_vline(xintercept = min(kaggle_df$GrLivArea)) +
  labs(title = "Above Ground Living Area (square feet)", align='middle') +
  theme_minimal()
```

A few takeaways when comparing the distributions:  

+ The original data for `GrLivArea` appears to be shaped as a more normal distribution, albeit right skewed, the most frequent square footage tends to be around 1500 and 1650 square feet.  

+ Alternatively, the exponential distribution of `GrLiveArea` appears to be even more right skewed, and takes on more of an exponential shape with a longer tail.

***

##### Find the 5th and 95th percentiles using CDF

Next, I'll find the 5th and 95th percentiles using the exponential cumulative distribution function (CDF). Thinking back to the formula from our reading, this function is:  

$$F(x) = 1 - e^{-\lambda x}$$
Since we already have found $\lambda = 0.000659864$, we can solve the equation accordingly for the 5th percentile:

$$0.05 = 1 - e^{-0.000659864x}$$
$$-0.95 = e^{-0.000659864x}$$
$$-ln(0.95) = 0.000659864x$$
$$\frac{-ln(0.95)}{0.000659864} = x$$
$$77.73313 = x$$
Similarly, we can find the 95th percentile by:  


$$0.95 = 1 - e^{-0.000659864x}$$
$$-0.05 = e^{-0.000659864x}$$
$$-ln(0.05) = 0.000659864x$$
$$\frac{-ln(0.05)}{0.000659864} = x$$
$$4539.92 = x$$
We can check these calculations using the `qexp()` function in R:  
```{r}
edf_5 <- qexp(0.05, rate=est_rate)
edf_95 <- qexp(0.95, rate=est_rate)

edf_5
edf_95
```

It looks like these match the calculations above!  

***

##### Generate 95% confidence interval from empirical data

Assuming normality, we can generate a 95% confidence interval from the empirical data by utilizing the following function:  

$$\overline{X} \pm Z * \frac{\sigma}{\sqrt{n}}$$

Since we have all of this information available to us, we can solve by utilizing the following:  

+ $Z$ = 1.96 (standard for a 95% confidence interval)  

+ $\overline{X}$ = mean value of `GrLivArea`  

+ $\sigma$ = standard deviation of `GrLivArea` mean  

+ $n$ = sample size of `GrLivArea`  

```{r}
X_cap <- mean(kaggle_df$GrLivArea)
Z <- 1.96
sigma <- std(kaggle_df$GrLivArea)
n <- length(kaggle_df$GrLivArea)

upper_bound <- X_cap + Z * (sigma / sqrt(n))
lower_bound <- X_cap - Z * (sigma / sqrt(n))

upper_bound
lower_bound

mean(kaggle_df$GrLivArea)
median(kaggle_df$GrLivArea)
```
And below is the empirical 5th percentile and 95th percentile of the data:  

```{r}
emp_5 <- quantile(kaggle_df$GrLivArea, c(0.05, 0.95))[1]
emp_95 <- quantile(kaggle_df$GrLivArea, c(0.05, 0.95))[2]
```

```{r}
calc_df <- data.frame(method= c('Exponential pdf percentiles', 'Empirical percentiles'), "percentile_5th" = c(edf_5, emp_5), "percentile_95th" = c(edf_95, emp_95))

calc_df %>% 
  kable() %>% 
  kable_styling(bootstrap_options = 'striped')
```

**Solution:  My calculations for my exponential CDF percentiles and empirical percentiles can be summarized in the table above. Additionally, I found that when calculating a 95% confidence interval from my empirical data, I found this interval to be $[1488.51, 1542.42]$. I also calculated the mean and median from my empirical data, which equals $1515.46$ and $1464$ respectively. Given that our `GrLivArea` variable is right skewed, the 95% confidence interval that we produced is not very effective and doesn't map to the data very well. Our confidence interval should indicate that if we were to continuously sample houses from this area, with the same sample size of 1460, then we should expect the mean `GrLivArea` value to fall within our bounds of $[1488.51, 1542.42]$ about 95% of the time -- which isn't reflected well in the sense that the empirical median does not fall within this range, characteristic of a right skewed distribution. **  

**Additionally, we found that when using the exponential pdf in an attempt to fit our empirical distribution, we see a large disparity between the 5th percentile of our exponential function (77.73) and our empirical data (848.00). This is also true when looking at the 95th percentiles that were calculated, the exponential pdf yielding 4539 square feet, and the empirical data yielding 2466.1 square feet. These percentiles mean that 95% of the houses in our training dataset have above ground living areas that fall below these markers. Therefore, the exponential percentile suggests that 95% of the houses have above ground living areas that are less than or equal to 4539 square feet, while the empirical data tells us that 95% of the houses have above ground living areas of up to 2466.1 square feet. This is a large difference between the two, and shows that the exponential distribution we fit would not be a good estimator of our empirical data.**  

***

> *Modeling*: Build some type of multiple regression  model and submit your model to the competition board.  Provide your complete model summary and results with analysis.

***

##### Data processing before running the model
 
Before we get started on building our multiple regression model, I thought it would be helpful to take a closer look at the training dataset, and to hone in on a few variables that seem like good attributes to include in our analysis.  

Earlier, I developed a large number of correlation matrices and scatter plots to visualize interactions between some of the quantitative variables in the dataset. When looking back at some of this data, I thought the following attributes stood out as possible additions to our regression model:  

+ `1stFlrSF` (first floor square feet)  
+ `GrLivArea` (above ground living area square feet)  
+ `GarageArea` (size of garage in square feet)  
+ `TotalBsmtSF` (total square feet of basement area)    
+ `YearBuilt` (year house was built)  
+ `TotRmsAbvGrd` (total rooms above ground)  
+ `BedroomAbvGr` (bedrooms above ground)  
+ `BsmtUnfSF` (unfinished square feet of basement area)  
+ `YrSold` (year sold)  

Additionally, I thought a few of the categorical variables stood out as possible effective attributes for predicting the Sale Price:  

+ `LandSlope` (slope of property)
+ `HouseStyle` (style of dwelling)
+ `CentralAir` (does the house have central air conditioning)
+ `SaleType` (type of sale)
+ `SaleCondition` (condition of sale)
+ `Foundation` (type of foundation)  

With these variables of interest selected, I'll need to recode the categorical variables into quantitative values:  

```{r, warning=FALSE, message=FALSE}
kaggle_df$LandSlope <- recode(kaggle_df$LandSlope, Gtl=1, Mod=2, Sev=3)
kaggle_df$HouseStyle <- recode(kaggle_df$HouseStyle, "1.5Fin"=1, "1.5Unf"=2, "1Story"=3, "2.5Fin"=4, "2.5Unf"=5, "2Story"=6, "SFoyer"=7, "SLvl"=8)
kaggle_df$CentralAir <- recode(kaggle_df$CentralAir, N=1, Y=2)
kaggle_df$SaleType <- recode(kaggle_df$SaleType, "COD"=1, "Con"=2, "ConLD"=3, "ConLI"=4, "ConLw"=5, "CWD"=6, "New"=7, "Oth"=8, "WD"=9)
kaggle_df$SaleCondition <- recode(kaggle_df$SaleCondition, Abnorml=1, AdjLand=2, Alloca=3, Family=4, Normal=5, Partial=6)
kaggle_df$Foundation <- recode(kaggle_df$Foundation, BrkTil=1, CBlock=2, PConc=3, Slab=4, Stone=5, Wood=6)
```

Now, with the categorical values recoded to numeric values, I decided to take the training dataset and only select the columns needed for the analysis:  

```{r}
train_df <- kaggle_df %>% 
  dplyr::select(`1stFlrSF`, GrLivArea, GarageArea, TotalBsmtSF, YearBuilt, TotRmsAbvGrd, BedroomAbvGr, BsmtUnfSF, YrSold, LandSlope, HouseStyle, CentralAir, SaleType, SaleCondition, Foundation, SalePrice)
```

Finally, we can check for any null values. If there are null values, we'll have to impute them:  

```{r}
anyNA(train_df)
```

It looks like there aren't any null values in our training dataset, so we are now ready to start building our multiple regression model.

***

##### Building the multiple regression model

For a first step, I'll add in all of the variables that we have in our subsetted dataframe. I'll use backward elimination to remove out any variables with high p-values:  

```{r}
train_lm <- lm(data = train_df, SalePrice ~ `1stFlrSF` +`GrLivArea` + `GarageArea` + `TotalBsmtSF` + `YearBuilt` + `TotRmsAbvGrd` + `BedroomAbvGr` + `BsmtUnfSF` + `YrSold` + `LandSlope` + `HouseStyle` + `CentralAir` + `SaleType` + `SaleCondition` + `Foundation`)

summary(train_lm)
```
Then, using backward elimination, I decided to remove out variables with high p-values:  

```{r}
train_lm_2 <- step(train_lm, direction = 'backward', trace = FALSE)
summary(train_lm_2)
```

This process didn't seem to affect the R-squared value much. Therefore, to try and boost the R-squared value a bit, I'll look back and see if I can add a few more variables and do more testing: 

For this run, I decided to add in `Neighborhood`, `OpenPorchSF`, `KitchenAvbGr`, `ExterQual`, `RoofMatl`, `YearRemodAdd`, and `OverallQual`, while removing a few variables that seem to be showing colinearity -- `CentralAir`, `LandSlope`, `YearBuilt`, `GarageArea`, and `TotRmsAbvGr`.

```{r}
train_df <- kaggle_df %>% 
  dplyr::select(`1stFlrSF`, GrLivArea, GarageArea, TotalBsmtSF, YearBuilt, TotRmsAbvGrd, BedroomAbvGr, BsmtUnfSF, YrSold, LandSlope, HouseStyle, CentralAir, SaleType, SaleCondition, Foundation, SalePrice, OverallQual, Fireplaces, YearRemodAdd, LotArea, RoofMatl, ExterQual, Utilities, KitchenAbvGr, Neighborhood, OpenPorchSF)


train_lm <- lm(data = train_df, SalePrice ~ `GrLivArea` + `TotalBsmtSF` + `BsmtUnfSF` + `SaleCondition` + OverallQual +  YearRemodAdd + LotArea + RoofMatl + ExterQual + KitchenAbvGr + Neighborhood + OpenPorchSF)

summary(train_lm)
```

This seemed to boost the R-squared value quite a lot. However, now there are a lot of variables in the output, and I'm worried that it may eventually overfit the testing data. To help cut down on this, I noticed that a few neighborhoods yielded very low p-values, and others not so much. Therefore, I recoded the Neighborhood variable accordingly:  

```{r}

kaggle_df$Neighborhood <- recode(kaggle_df$Neighborhood,
                                 Blmngtn = 1,
                                 Blueste = 1,
                                 BrDale = 1,
                                 BrkSide = 1,
                                 ClearCr = 1,
                                 CollgCr = 1,
                                 Crawfor = 1,
                                 Edwards = 1,
                                 Gilbert = 1,
                                 IDOTRR = 1,
                                 MeadowV = 1,
                                 Mitchel = 1,
                                 NAmes = 1,
                                 NoRidge = 2,
                                 NPkVill = 1,
                                 NridgHt = 3,
                                 NWAmes = 1,
                                 OldTown = 1,
                                 Sawyer = 1,
                                 SawyerW = 1,
                                 Somerst = 1,
                                 StoneBr = 4,
                                 SWISU = 1,
                                 Timber = 1,
                                 Veenker = 1)

table(kaggle_df$Neighborhood)
```
```{r}
train_df <- kaggle_df %>% 
  dplyr::select(`1stFlrSF`, GrLivArea, GarageArea, TotalBsmtSF, YearBuilt, TotRmsAbvGrd, BedroomAbvGr, BsmtUnfSF, YrSold, LandSlope, HouseStyle, CentralAir, SaleType, SaleCondition, Foundation, SalePrice, OverallQual, Fireplaces, YearRemodAdd, LotArea, RoofMatl, ExterQual, Utilities, KitchenAbvGr, Neighborhood, OpenPorchSF)

train_lm <- lm(data = train_df, SalePrice ~ `GrLivArea` + `TotalBsmtSF` + `BsmtUnfSF` + `SaleCondition` + OverallQual +  YearRemodAdd + LotArea + RoofMatl + ExterQual + KitchenAbvGr + Neighborhood)

summary(train_lm)
```

With this model above, we are able to capture about 85% of the variability in `SalePrice` from these attributes that make up the multiple regression. To be safe, we'll do a bit of testing on the residuals to ensure that this is a valid regression model:  


***

##### Evaluating the Model and Residual Analysis


After running the summary above, we can see a few things:

+ The median residual value is -554, which given the spread of the dataset, it is roughly around zero, which is a good sign.  

+ Additionally, the minimum and maximum values of the residuals are roughly around the same, albeit leaning a bit more on the minimum side at -398985 and 259408 respectively.  

+ Our Multiple R-squared value is 0.8493, which indicates that these terms account for about 85% of the variability in the Sale Price of the homes in this dataset. This seems to be a pretty good result for our regression model.  


Although this seems to be a good sign, we need to check our residuals to see if this qualifies as a suitable regression model.

When plotting residuals (below), we can see that the residuals are pretty uniformly scattered around zero:  
```{r}
plot(fitted(train_lm),resid(train_lm))
```

And finally, the residuals when plotted on a q-q plot appear to be pretty normally distributed, although a bit skewed at the poles. We can see from the histogram that the distribution is unimodal and is slightly left skewed, however, it is approaching normal so I feel that this qualifies as a valid multiple regression model.  

```{r}
qqnorm(resid(train_lm))
qqline(resid(train_lm))

hist(resid(train_lm))
```

***

##### Importing the test data and making predictions with multiple regression model

```{r, warning=FALSE, message=FALSE}
test_df <- read_csv('https://raw.githubusercontent.com/zachalexander/data605_cuny/master/Final%20Exam/test.csv')
```


```{r, echo=FALSE, include=FALSE}
test_df$Neighborhood <- recode(test_df$Neighborhood,
                                 Blmngtn = 1,
                                 Blueste = 1,
                                 BrDale = 1,
                                 BrkSide = 1,
                                 ClearCr = 1,
                                 CollgCr = 1,
                                 Crawfor = 1,
                                 Edwards = 1,
                                 Gilbert = 1,
                                 IDOTRR = 1,
                                 MeadowV = 1,
                                 Mitchel = 1,
                                 NAmes = 1,
                                 NoRidge = 2,
                                 NPkVill = 1,
                                 NridgHt = 3,
                                 NWAmes = 1,
                                 OldTown = 1,
                                 Sawyer = 1,
                                 SawyerW = 1,
                                 Somerst = 1,
                                 StoneBr = 4,
                                 SWISU = 1,
                                 Timber = 1,
                                 Veenker = 1)

table(test_df$Neighborhood)

test_df$SaleCondition <- recode(test_df$SaleCondition, Abnorml=1, AdjLand=2, Alloca=3, Family=4, Normal=5, Partial=6)

table(test_df$SaleCondition)

```


```{r, fig.width=12, fig.height=4}
sale_predictions <- predict(train_lm, test_df) 
par(mfrow=c(1,2))
hist(sale_predictions, breaks=40, xlim=c(0, 600000), main = 'predicted sale price - test data')
hist(train_df$SalePrice, breaks=50, xlim=c(0, 600000), main = 'sales price - training data')
```

As we can see when comparing the two distributions, they both seem to be roughly the same shape -- which is a good indication that our predicted values are similar to our training values. Now, we can get this model ready to submit to Kaggle.

##### Report your Kaggle.com user name and score

```{r}
kaggle_data <- data.frame(id = test_df[,"Id"],  SalePrice = sale_predictions)
kaggle_data[kaggle_data < 0] <- 0
kaggle_data <- replace(kaggle_data, is.na(kaggle_data), 0)
write.csv(kaggle_data, file="kaggle.csv", row.names = FALSE)
```

Overall, I fell towards the middle of the pack on the leaderboard, but hope to work on this more in the future. It would be great to set this up to run SVM and/or Random Forest to see if I can improve my score.

***

##### My Kaggle Score:  

```{r}
kaggle_info <- c("zdalexander", 0.42696)
names(kaggle_info) <- c("Username", "My Score")
kable(kaggle_info, col.names = "Kaggle") %>% 
  kable_styling(full_width = F)
```



##### My YouTube Link Explaining My Final:  

You can find my YouTube video here: [https://www.youtube.com/watch?v=pvlqQwg9cag](https://www.youtube.com/watch?v=pvlqQwg9cag)

Thanks!
