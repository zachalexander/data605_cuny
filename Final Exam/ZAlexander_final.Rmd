---
title: "DATA 605 - Final Project"
author: "Zach Alexander"
date: "5/17/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(dplyr)
require(tidyr)
require(kableExtra)
require(knitr)
require(psych)
```

***

### Instructions

Your final is due by the end of the last week of class. You should post your solutions to your GitHub account or RPubs. You are also expected to make a short presentation via YouTube and post that recording to the board. This project will show off your ability to understand the elements of the class.

### Problem 1

***
> Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6. Then generate a random variable Y that has 10,000 random normal numbers with a mean of $\mu=\sigma=\frac{(N+1)}{2}$

***

Following the instructions from above, here is my R syntax to generate a random variable X:  

```{r}
set.seed(123)

N <- 10
X <- runif(10000, 1, N)
Y <- rnorm(10000, (N+1)/2, (N+1)/2)
```

From the above syntax, you can see that I've created a variable $X$ with 10,000 random uniform numbers from 1 to $N$ ($N=10$). Additionally, I've created a random variable $Y$ that has 10,000 random normal numbers with a $\mu$ and $\sigma$ of $\frac{N+1}{2}$. 

***

> *Probability*: Calculate as a minimum the below probabilities a through c. Assume the small letter $x$ is estimated as the median of the $X$ variable, and the small letter $y$ is estimated as the 1st quartile of the $Y$ variable.  Interpret the meaning of all probabilities.  

***

To work through this, I'll first have to calculate $x$ (the median) and $y$ (the first quartile). Additionally, I've saved $X$ and $Y$ in a dataframe can stored the total number of rows (10,000) in a variable:  
```{r}
x <- quantile(X, 0.50)
y <- quantile(Y, 0.25)
df <- data.frame(X = X, Y = Y)
total_rows <- nrow(df)
```

***

> *a)* $P(X>x \ | \ X>y)$

***

We can use the following equation to find the probability:  

$$P(A \ | \ B) = \frac{P(A \ \cap \ B)}{P(B)} \\ where \ A = P(X>x) \ and \ B = P(X>y)$$  

With this in mind, we can solve:  
```{r}
P_AandB <- nrow(df %>% filter(X > x & X > y)) / total_rows
P_B <- nrow(df %>% filter(X > y)) / total_rows

P_AgivenB <- P_AandB / P_B
P_AgivenB
```

**Solution: The probability $P(X>x \ | \ X>y) = 0.5510$, which means that the probability that a uniform number from 1 to 10 is greater than the median of 5.45 *given* this number is greater than 1.84 is 0.5510.**  

***

> *b)* $P(X>x, \ Y>y)$  

***

We can use the following equation to solve this portion:  

$$P(A,B)=P(A) \times P(B) \\ where \ A=N(X>x) \ and \ B=N(Y>y)$$  

With this in mind, we can solve:  
```{r}
P_AandB_2 <- nrow(df %>% filter(X > x & Y > y)) / total_rows
P_AandB_2
```

**Solution: The probability $P(X>x, \ Y>y) = 0.3756$, which means that the probability that a uniform number from 1 to 10 is greater than the median of 5.45 *and* this number is greater than 1.84 is 0.3756.**  

***
> *c)* $P(X<x, \ X>y)$  

***

Similar to part a, we can use the following equation to solve:  

$$P(A \ | \ B) = \frac{P(A \ \cap \ B)}{P(B)} \\ where \ A = P(X<x) \ and \ B = P(X>y)$$  

\br

With this in mind, we can solve:  

```{r}
P_AandB <- nrow(df %>% filter(X < x & X > y)) / total_rows
P_B <- nrow(df %>% filter(X > y)) / total_rows

P_AgivenB_2 <- P_AandB / P_B
P_AgivenB_2
```

\br

**Solution: The probability $P(X<x \ | \ X>y) = 0.4490$, which means that the probability that a uniform number from 1 to 10 is less than the median of 5.45 *given* this number is greater than 1.84 is 0.4490.**  

***

> Investigate whether $P(X>x \ and \ Y>y)=P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.  

***

To do this investigation, we'll first have to calculate the joint probabilities:  

We'll calculate joint probabilities by using the equation:  

$$P(A \cap B) = P(A \times B)$$
Therefore, we first will create a few columns to distinguish whether our values meet the following conditions:  

+ Is $X > x$ or $X < x$?
+ Is $Y > y$ or $Y < y$?

Then, we can take the count of each condition, and multiply the four conditions to get their respective joint probabilities:  

+ $P((X > x) \times (Y < y))$
+ $P((X < x) \times (Y < y))$
+ $P((X < x) \times (Y > y))$
+ $P((X > x) \times (Y > y))$

Below is the R syntax I used to do this, by using `group_by()` and `summarise()` functions to do the calculations with the `tidyverse` package:  

```{r}
# Joint probabilities
prob_df <- df %>% 
  mutate(A = ifelse(X > x, " X > x", " X < x"), B = ifelse(Y > y, " Y > y", " Y < y")) %>% 
  group_by(A, B) %>% 
  summarise(count = n()) %>% 
  mutate(prob = count / total_rows)
```

Next, we need to back up a bit and examine the marginal probabilities of $P(A)$ and $P(B)$ independently:  

Intuitively we know that $x = median(X)$, therefore, half of the values of $X$ > $x$ and the other half of $X$ < $x$. Similarly, since we know that $y = first \ quartile \ of \ Y$, a quarter of $Y$ < $y$ and the remaining three fourths of the values of $Y$ > $y$. Therefore, the marginal probabilities are:  

+ $P(X > x) = 0.50$  
+ $P(X < x) = 0.50$  
+ $P(Y > y) = 0.75$  
+ $P(Y < y) = 0.25$  

We can check these assumptions by reworking our table:  

```{r}
prob_df <- prob_df %>% 
  ungroup() %>% 
  group_by(A) %>% 
  summarise(count = sum(count), prob = sum(prob)) %>% 
  mutate(B = "Total Prob") %>% 
  bind_rows(prob_df)

prob_df <- prob_df %>% 
  ungroup() %>% 
  group_by(B) %>% 
  summarise(count = sum(count), prob = sum(prob)) %>% 
  mutate(A = "Total Prob") %>% 
  bind_rows(prob_df)
```


And by printing our table, we can see that our marginal probabilities that we assumed are confirmed:  

```{r}
prob_df %>%
  select(-count) %>% 
  spread(A, prob) %>%
  rename("Conditions" = B) %>%
  kable() %>%
  kable_styling(bootstrap_options = c('striped'), full_width = FALSE)
```

**Solution: After generating the table above, it appears that $P(X>x \ and \ Y>y)$ does equal $P(X>x)P(Y>y)$, since they both have a value of 0.3756. By doing the calculations to check, $P(X>x)P(Y>y) = (0.50)(0.75) = 0.375$ and $P(X>x \ and \ Y>y) = 0.375$ as well.**  

***

> Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test. What is the difference between the two? Which is most appropriate?

***

In order to run our Fisher's Exact Test and Chi Square Test, we'll first need to find the counts of each our conditions, similar to the previous question and create a two-by-two table:  

```{r}
X_plus <- nrow(df %>% 
  filter(X > x))

X_lesseq <- nrow(df %>% 
  filter(X <= x))

Y_lesseq <- nrow(df %>% 
  filter(Y <= y))

Y_plus <- nrow(df %>% 
  filter(Y > y))

freq_matrix <- matrix(c(X_plus, X_lesseq, Y_plus, Y_lesseq), 
                      nrow = 2, ncol = 2, byrow = TRUE,
                      dimnames = list(c("x", "y"),
                                c("X > x; Y > y", "X <= x; Y <= y")))
```

With our frequency table ready to go, we can see our counts below:

```{r}
freq_matrix %>% 
  kable() %>%
  kable_styling(bootstrap_options = c('striped'), full_width = FALSE)
```

Now, we are ready to run our Fisher's Exact Test:  

```{r}
fisher.test(freq_matrix)
```

And here is the Chi-Square Test:  

```{r}
chisq.test(freq_matrix)
```

When looking at both of the outputs, we can see that we reject the null hypothesise for both, given that the p-value is less than 0.05 for the 95 percent confidence interval. Since the Fisher's Exact Test is typically used for smaller sample sizes, and our sample is of 10,000 observations, it would be more appropriate to use the Chi-Square Test. 

**Solution: **

+ **Our $H_0$ = There is no relationship between X and Y.**  

+ **Our $H_a$ = There is a significant relationship between X and Y.**  

**Given our outputs, independence does not hold when we run the Fisher's Exact Test and Chi-Square test. We reject the null hypothesis that there is no relationship between X and Y. Since the Fisher's Exact Test is typically used on smaller sample sizes, and we have a large sample size of 10,000 observations, it would be more appropriate to rely on the output from our Chi-Square Test. **  

***

### Instructions

You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques]([https://www.kaggle.com/c/house-prices-advanced-regression-techniques). I want you to do the following.

### Problem 2

***

> *Descriptive and Inferential Statistics*: Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?

***

```{r, warning=FALSE, message=FALSE}
kaggle_df <- read_csv('https://raw.githubusercontent.com/zachalexander/data605_cuny/master/Final%20Exam/train.csv')
```

```{r}
# summary(kaggle_df)
```

On a quick look at the summary, I decided to pull out the numeric values to explore a bit more:
```{r}
numerics <- data.frame(summary(kaggle_df))

numerics <- numerics %>% 
  filter(!is.na(Freq))

numerics <- data.frame(numerics %>% 
  filter(!grepl('Class', Freq), !grepl('Length', Freq), !grepl('Mode', Freq)) %>% 
  separate(Freq, c('Value', 'Count'), sep = ":", remove = TRUE))

numerics$Var2 <- trimws(numerics$Var2)
numerics$Value <- trimws(numerics$Value)
numerics$Count <- round(as.numeric(trimws(numerics$Count)), 1)

numerics <- numerics %>%
  select(Var2, Value, Count) %>%
  spread(Value, Count) %>% 
  rename("Attribute" = "Var2", "Q1" = `1st Qu.`, "Q3" = `3rd Qu.`, "Max" = "Max.", "Min" = "Min.", "NA" = "NA's") %>% 
  select(Attribute, Min, Q1, Median, Mean, Q3, Max, `NA`)

kable(numerics) %>% 
  kable_styling(bootstrap_options = 'striped')
```

With my `numerics` dataframe ready, I can now set up some visuals by splitting the dataframe into two, one with all quantative variables, and the other with categorical data:  

```{r}
kaggle_numerics <- kaggle_df[, (colnames(kaggle_df)) %in% numerics$Attribute]
kaggle_categorical <- kaggle_df[, !(colnames(kaggle_df) %in% numerics$Attribute)]
```


***

##### Exploring the quantitative variables

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(2:11) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```
```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(12:21) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(22:31) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

```{r, fig.width=12, fig.height=12}
kaggle_numerics %>% 
  dplyr::select(32:38) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")
```

After creating a few plot matrices with our quantitative data, I thought there were a few initial takeaways:  

+ Many of the living spaces, and the square footage values seem to show stronger correlations with one another. This makes sense given that many times, the square footage of one floor will likely be similar square footage to another floor in the house.  For instance, there's a strong correlation between the first floor square footage and the basement square footage. When thinking about our regression later, it's nice to know that there are multiple attributes here that could serve as proxies (if needed).

+ Additionally, I'm seeing a gradual, positive trend towards larger areas of garages the later they are built in the 20th century -- for instance, garages built in the early part of the 20th century seem to be smaller than those that are built in the later 1900s and early 2000s. This may be something to look into further, and it's relationship with the Sale Price later on.  

+ It's interesting to see too that the year the house is built has a relationship with the rating of overall material and finish quality of the house. Again, this make sense, but it would be valueable to see if this has a relationship with Sale Price.

Now, let's explore the categorical variables a bit.   


***

##### Exploring a few of the categorical variables

```{r}
table(kaggle_categorical$LandSlope)
# table(kaggle_categorical$BldgType)
table(kaggle_categorical$HouseStyle)
# table(kaggle_categorical$RoofStyle)
table(kaggle_categorical$RoofMatl)
# table(kaggle_categorical$ExterCond)
table(kaggle_categorical$CentralAir)
# table(kaggle_categorical$Utilities)
table(kaggle_categorical$HeatingQC)
table(kaggle_categorical$SaleType)
table(kaggle_categorical$SaleCondition)
table(kaggle_categorical$Foundation)
```

After looking through a large number of categorical values, and checking frequencies, a few things stood out to me:  

+ It looks like there's a majority of houses that are built on gentle slopes, however, there's a proportion that are built on moderate or severe slopes -- this could be something to look into further when building out the regression later on. It may be interesting to see if there is any correlation here with the Sale Price.  

+ Similarly, the foundation material seems to be split heavily between cinder blocks and poured concrete. This is another feature that may be useful to explore later to see if this has an affect on housing prices, or if there is a connection between the year a house was built and which type of foundation was used.  

+ Finally, Sale Type and Sale Condition were interesting values and their frequencies suggest that there isn't a "one-size-fits-all" approach to certain transactions, making this an enticing set of features to explore further.  

***

##### Developing a scatterplot matrix

After plotting all of my quantitative variables earlier, I thought I'd hone in on a few that may be useful to explore their relationship with the Sale Price variable. You can find my plot below: 

```{r, fig.width=12, fig.height=12}
pairs(kaggle_df[, c('1stFlrSF', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'YearBuilt', 'SalePrice' )], pch = 19)
```

*** 
##### Developing a correlation matrix  

For our correlation matrix, I will use `GrLivArea`, `1stFlrSF`, and `SalePrice`. And taking a few of these same attributes, I thought it would be interesting to plot these in a correlation matrix:  

```{r}
kaggle_numerics %>% 
  dplyr::select(`1stFlrSF`, `GrLivArea`, SalePrice) %>%
  pairs.panels(method = "pearson", hist.col = "#8bb397")

corr_matrix <- kaggle_numerics %>% 
  select(`1stFlrSF`, GrLivArea, SalePrice) %>% 
  cor() %>% 
  as.matrix()
```

As we can see from above, all three attributes, the square footage of the first floor, the above grade (ground) living area and the Sale Price all seem to have a positive relationship with one another. All three distributions also appear to be unimodal and right-skewed. Below is the official correlation matrix:  

```{r}
kable(corr_matrix) %>% 
  kable_styling(bootstrap_options = 'striped', full_width = FALSE)
```

***

##### Hypothesis testing on correlation matrix 

Now, although these correlations seem to be quite strong, we can do some hypothesis testing on their relationships at an 80-percent confidence interval:  

```{r}
corr1 <- cor.test(kaggle_numerics$SalePrice, kaggle_numerics$`1stFlrSF`, method = 'pearson', conf.level = 0.80)

corr2 <- cor.test(kaggle_numerics$SalePrice, kaggle_numerics$GrLivArea, method = 'pearson', conf.level = 0.80)

corr3 <- cor.test(kaggle_numerics$`1stFlrSF`, kaggle_numerics$GrLivArea, method = 'pearson', conf.level = 0.80)

corr1
corr2
corr3
```

**Solution: After running the three pairwise correlations between my variables of `SalePrice`, `1stFlrSF`, and `GrLivArea`, I can see from the outputs that all three accept the alternative hypothesis that the true correlation is not equal to zero and that there is a positive relationship between these attributes. Therefore, this rejects the null hypothesis that states that the correlations between each pairwise set of variables is zero. I know this to be true since the p-values for all three of my Person's correlation tests are less than 0.05.**  

**Solution: Additionally, since we are dealing with a large sample size, and our correlation tests are yielding very small p-values, all less than 0.001, I'm not worried about family-wise error when measuring relationships across these three attributes. With p-values this low, it creates a solid argument that we aren't running into a Type 1 error here.**  

***

> *Linear Algebra and Correlation*: Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. 

***

Remembering that the inverse of a matrix multiplied by a matrix will become the identity matrix: 

$$A^{−1}A=AA^{−1}=I$$

We can find our precision matrix by using the `solve()` function in baseR to invert our correlation matrix:  

```{r}
prec_matrix <- solve(corr_matrix)
prec_matrix
```

Then, we can multiple the precision matrix by the correlation matrix to confirm that we get the identity matrix:  

$$ correlation \ matrix \times \ precision \ matrix = 

\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{bmatrix} $$

```{r}
I_1 <- round(prec_matrix %*% corr_matrix, 2)
I_2 <- round(corr_matrix %*% prec_matrix, 2)

I_1
I_2
```

As we can see above, we do indeed get the Identity Matrix when we multiply the precision matrix with the correlation matrix and vice-versa.  

Finally, we will perform LU decomposition on the matrix:  


